{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010e33a4-9ac6-4e3b-8085-44f7cfdb2eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pygame\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import queue\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Audio synthesis constants\n",
    "SAMPLE_RATE = 44100\n",
    "CHUNK_SIZE = 1024\n",
    "NYQUIST = SAMPLE_RATE / 2\n",
    "\n",
    "@dataclass\n",
    "class HandData:\n",
    "    landmarks: List\n",
    "    is_right: bool\n",
    "    bbox: Tuple[int, int, int, int]\n",
    "\n",
    "class AdvancedFMSynth:\n",
    "    \"\"\"Real-time FM polyphonic synthesizer with effects.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.is_recording = False\n",
    "        self.recording_buffer = []\n",
    "        \n",
    "        # FM synthesis parameters\n",
    "        self.voices = {}\n",
    "        self.carrier_freqs = {}\n",
    "        self.modulator_freqs = {}\n",
    "        self.volume_envelope = {}\n",
    "        \n",
    "        # Effects\n",
    "        self.reverb_time = 0.3\n",
    "        self.delay_time = 0.2\n",
    "        self.distortion = 0.0\n",
    "        \n",
    "    def fm_synth_sample(self, carrier_freq: float, mod_freq: float, \n",
    "                       mod_index: float = 5.0, duration: float = 1/60) -> np.ndarray:\n",
    "        \"\"\"Generate one frame of FM synthesis.\"\"\"\n",
    "        t = np.linspace(0, duration, int(SAMPLE_RATE * duration), False)\n",
    "        \n",
    "        # Carrier + modulator\n",
    "        modulator = np.sin(2 * np.pi * mod_freq * t)\n",
    "        carrier = np.sin(2 * np.pi * carrier_freq * t + mod_index * modulator)\n",
    "        \n",
    "        # ADSR envelope (simple decay)\n",
    "        envelope = np.exp(-5 * t)\n",
    "        return carrier * envelope\n",
    "    \n",
    "    def apply_effects(self, samples: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Reverb + delay + distortion.\"\"\"\n",
    "        output = samples.copy().astype(np.float32)\n",
    "        \n",
    "        # Distortion\n",
    "        if self.distortion > 0:\n",
    "            output = np.tanh(output * (1 + self.distortion))\n",
    "        \n",
    "        # Simple delay\n",
    "        delay_samples = int(self.delay_time * SAMPLE_RATE)\n",
    "        if len(output) > delay_samples:\n",
    "            output[delay_samples:] += 0.3 * output[:-delay_samples]\n",
    "        \n",
    "        # Reverb (comb filter)\n",
    "        reverb_samples = int(self.reverb_time * SAMPLE_RATE)\n",
    "        if len(output) > reverb_samples:\n",
    "            output[reverb_samples:] += 0.2 * output[:-reverb_samples]\n",
    "        \n",
    "        return np.clip(output, -1.0, 1.0)\n",
    "    \n",
    "    def generate_audio(self):\n",
    "        \"\"\"Audio callback - generates continuous stream.\"\"\"\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=pyaudio.paFloat32,\n",
    "                       channels=1,\n",
    "                       rate=SAMPLE_RATE,\n",
    "                       output=True,\n",
    "                       frames_per_buffer=CHUNK_SIZE)\n",
    "        \n",
    "        while True:\n",
    "            chunk = np.zeros(CHUNK_SIZE, dtype=np.float32)\n",
    "            \n",
    "            # Mix active voices\n",
    "            for voice_id, (freq, mod_freq) in self.voices.items():\n",
    "                voice_samples = self.fm_synth_sample(freq, mod_freq)\n",
    "                chunk[:len(voice_samples)] += voice_samples[:CHUNK_SIZE]\n",
    "            \n",
    "            # Apply effects and normalize\n",
    "            chunk = self.apply_effects(chunk)\n",
    "            chunk /= max(1, len(self.voices))\n",
    "            \n",
    "            # Record if active\n",
    "            if self.is_recording:\n",
    "                self.recording_buffer.extend(chunk)\n",
    "            \n",
    "            stream.write(chunk.astype(np.float32).tobytes())\n",
    "        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "class GestureOrchestra:\n",
    "    \"\"\"Advanced gesture-to-music mapping.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.synth = AdvancedFMSynth()\n",
    "        self.chord_progressions = {\n",
    "            'pop': ['C', 'G', 'Am', 'F'],\n",
    "            'punk': ['G', 'D', 'Em', 'C'], \n",
    "            'jazz': ['Cmaj7', 'Dm7', 'G7', 'Cmaj7'],\n",
    "            'sad': ['Am', 'F', 'C', 'G']\n",
    "        }\n",
    "        self.current_progression = 0\n",
    "        self.progression_step = 0\n",
    "        \n",
    "    def fingers_up(self, landmarks: List) -> List[bool]:\n",
    "        \"\"\"Advanced finger detection with hysteresis.\"\"\"\n",
    "        fingers = []\n",
    "        # Thumb (x-axis)\n",
    "        fingers.append(landmarks[4].x < landmarks[3].x * 0.97)\n",
    "        # Other fingers (y-axis with smoothing)\n",
    "        for tip, pip in [(8,6), (12,10), (16,14), (20,18)]:\n",
    "            fingers.append(landmarks[tip].y < landmarks[pip].y * 0.98)\n",
    "        return fingers\n",
    "    \n",
    "    def hand_position(self, landmarks: List, frame_shape: Tuple) -> Dict:\n",
    "        \"\"\"3D-aware hand position and gesture parameters.\"\"\"\n",
    "        h, w = frame_shape[:2]\n",
    "        wrist = landmarks[0]\n",
    "        \n",
    "        # Normalized position\n",
    "        pos = {\n",
    "            'x': wrist.x,\n",
    "            'y': wrist.y,\n",
    "            'z': wrist.z,  # Depth\n",
    "            'pitch': (wrist.y - 0.5) * 12,  # Height = pitch bend\n",
    "            'spread': self._finger_spread(landmarks)  # Vibrato\n",
    "        }\n",
    "        return pos\n",
    "    \n",
    "    def _finger_spread(self, landmarks: List) -> float:\n",
    "        \"\"\"Finger spread for vibrato.\"\"\"\n",
    "        tips = [landmarks[i] for i in [8,12,16,20]]\n",
    "        distances = []\n",
    "        for i in range(4):\n",
    "            for j in range(i+1, 4):\n",
    "                dist = np.sqrt((tips[i].x-tips[j].x)**2 + (tips[i].y-tips[j].y)**2)\n",
    "                distances.append(dist)\n",
    "        return np.mean(distances)\n",
    "    \n",
    "    def recognize_gesture(self, fingers: List[bool], hand_pos: Dict, is_right: bool) -> Dict:\n",
    "        \"\"\"Advanced gesture recognition with context.\"\"\"\n",
    "        fingers_str = ''.join('1' if f else '0' for f in fingers)\n",
    "        \n",
    "        gestures = {\n",
    "            '00000': 'stop',  # Fist\n",
    "            '10000': 'record_toggle' if is_right else 'progression_next',  # Thumb\n",
    "            '00001': 'fx_cycle',  # Pinky\n",
    "            '11111': 'open_chord',  # Palm\n",
    "            '00100': 'bass_note',  # Middle finger\n",
    "            '10101': 'arpeggio',  # Multiple fingers\n",
    "        }\n",
    "        \n",
    "        gesture = gestures.get(fingers_str, 'sustain')\n",
    "        return {\n",
    "            'type': gesture,\n",
    "            'pitch_bend': hand_pos['pitch'],\n",
    "            'vibrato': hand_pos['spread'] * 2,\n",
    "            'hand': 'right' if is_right else 'left'\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Initialize\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(max_num_hands=2, model_complexity=1, min_detection_confidence=0.8)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    orchestra = GestureOrchestra()\n",
    "    \n",
    "    # Start audio thread\n",
    "    audio_thread = threading.Thread(target=orchestra.synth.generate_audio, daemon=True)\n",
    "    audio_thread.start()\n",
    "    \n",
    "    print(\"üéº ADVANCED AIR GESTURE ORCHESTRA\")\n",
    "    print(\"Left Hand: Chords/Bass | Right Hand: Melody/Solo\")\n",
    "    print(\"Gestures: Fist=Stop, Thumb=Record/Prog, Palm=Chords, Fingers=Notes\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "        \n",
    "        active_voices = {}\n",
    "        hand_data = []\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Determine handedness (chiral test)\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                is_right = wrist.x > 0.5  # Simple heuristic\n",
    "                \n",
    "                # Get gesture data\n",
    "                fingers = orchestra.fingers_up(hand_landmarks.landmark)\n",
    "                pos = orchestra.hand_position(hand_landmarks.landmark, frame.shape)\n",
    "                gesture = orchestra.recognize_gesture(fingers, pos, is_right)\n",
    "                \n",
    "                hand_data.append(HandData(hand_landmarks.landmark, is_right, (0,0,0,0)))\n",
    "                \n",
    "                # Map to notes based on hand role\n",
    "                if gesture['type'] == 'open_chord' and not is_right:\n",
    "                    # Left hand chords\n",
    "                    chord_notes = [60, 64, 67]  # C major base\n",
    "                    for j, note in enumerate(chord_notes):\n",
    "                        freq = 440 * (2 ** ((note + gesture['pitch_bend']) / 12))\n",
    "                        active_voices[f\"L{j}\"] = (freq, freq * 0.25)  # FM params\n",
    "                \n",
    "                elif gesture['type'] == 'bass_note' and not is_right:\n",
    "                    # Left hand bass\n",
    "                    bass_freq = 130.81 + gesture['pitch_bend'] * 5  # C2 base\n",
    "                    active_voices[\"bass\"] = (bass_freq, bass_freq * 2)\n",
    "                \n",
    "                elif gesture['type'] == 'arpeggio' and is_right:\n",
    "                    # Right hand melody\n",
    "                    melody_notes = [67, 72, 76, 79]  # G major arpeggio\n",
    "                    note_idx = int(pos['x'] * len(melody_notes)) % len(melody_notes)\n",
    "                    freq = 440 * (2 ** ((melody_notes[note_idx] + gesture['pitch_bend']) / 12))\n",
    "                    active_voices[\"melody\"] = (freq, freq * 3)  # Higher mod for lead\n",
    "                \n",
    "                # Update synth\n",
    "                orchestra.synth.voices = active_voices\n",
    "                orchestra.synth.distortion = pos['spread'] * 0.5\n",
    "        \n",
    "        # Visual feedback\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (10, 10), (400, 200), (0,0,0), -1)\n",
    "        cv2.putText(overlay, f\"Voices: {len(active_voices)}\", (15, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "        cv2.putText(overlay, f\"Recording: {'‚óè' if orchestra.synth.is_recording else '‚óã'}\", \n",
    "                   (15, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,255), 2)\n",
    "        \n",
    "        if hand_data:\n",
    "            for hand in hand_data[:2]:\n",
    "                mp_drawing.draw_landmarks(overlay, hand.landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "        cv2.imshow('Advanced Air Gesture Orchestra üéº', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76d2cc-34f9-44af-ac00-2b28b0a559a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
